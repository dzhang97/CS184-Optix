<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
</style>
<title>CS 184 Optix</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">
</head>


<body>

<h1 align="middle">CS 184: Computer Graphics and Imaging, Spring 2018</h1>
<h1 align="middle">Final Report Webpage</h1>
<h2 align="middle">Keming Kao, Denis Li, David Zhang</h2>

<h2 align="middle"></h2>

<h2 align="middle">Nvidia OptiX as a Tool to Accelerate Path Tracing</h2>

<h2 align="middle">Light Field Cameras</h2>
<p>As a group, we agreed to change our project completely from our original plan (Nvidia OptiX to optimize path tracing), as we did not realize that using the Nvidia OptiX API would require having a Nvidia GPU in our system. One group member did have a GPU, but building the pathtracer project on Windows proved to be a whole project on its own, so we decided to switch to the light field camera project that built off the existing pathtracer code. Our objective was to emulate the multilens array shown in lecture to simulate a plenoptic camera. To do so, we had to make the assumption that pinhole images looked like subpixel images, and leveraged command line arguments for easy camera shifting. Using our grid of images, we could recreate images using Python and the shift and add algorithm presented in class. Playing around with this allowed for refocusing at different depths. As a final touch, we implemented antialiasing by taking a larger grid of subpixel images for the final, summed image.</p>

<h2 align="middle">Technical Approach</h2>

<h3 align="middle">Varying Camera Position</h3>
<p>To achieve multiple shots in a localized area through our pinhole camera model, we needed an efficient way to render multiple images on multiple machines simultaneously, so one of the first goals we accomplished was enabling command line arguments for changing the camera's position and its target location (where it was pointing at). Through some simple code digging we created simple calls from main to application to camera that set the spherical coordinates of the camera and target location, as well as supporting printing out these coordinates in the edit mode of the renderer. We also printed out the X and Y normals to the camera direction from the c2w member of camera to be able to add shifts to our camera position while being on the same XY plane.</p>

<h3 align="middle">Microlenses and Sub-Aperture Images</h3>
<p>With the ability to set up renders in a small, localized area in the input image, we could now simulate a microlens by taking individual shots in a NxN grid with set offsets. The offsets were generated by shifting by some set amount in the X normal and Y normal directions.</p>
<div align="center">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/sal.png" align="middle" width="360px"/>
          <figcaption align="middle">This would be a render taken slightly to the left</figcaption>
      </td>
      <td>
        <img src="images/sar.png" align="middle" width="360px"/>
          <figcaption align="middle">This would be a render taken slightly to the right</figcaption>
     </td>
    </tr>
  </table>
</div>
<p><i>[Image courtesy of Light Field Cameras II Lecture]</i></p>


<h3 align="middle">Focusing, Depth of Field, Aperture Size</h3>
<p>We found that with just a 3x3 example, we were able to accomplish the "shift and add" algorithm explained in lecture to focus near or far in the scene. To get a more precise and adjustable focus, we leveraged bilinear interpolation between the shifts to achieve non-integer shift results. Initially, the focus is set at infinity (images of the nxn grid stacked directly on top of each other). As we pulled these images apart, the focus came in closer:</p>
<div align="center">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/far.png" align="middle" width="360px"/>
      </td>
      <td>
        <img src="images/near.png" align="middle" width="360px"/>
     </td>
    </tr>
  </table>
</div>
<p>We experimented with different small offsets between subpixel images to test effects on simulated aperture size and depth of field. Judging by the blurs in the sum of our NxN renders, we found that smaller shifts in the images generated a small aperture and large depth of field, while larger shifts generated a large aperture and small depth of field:</p>

<div align="center">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/smalla.png" align="middle" width="360px"/>
      </td>
      <td>
        <img src="images/mediuma.png" align="middle" width="360px"/>
     </td>
      <td>
        <img src="images/largea.png" align="middle" width="360px"/>
     </td>
    </tr>
  </table>
</div>

<h3 align="middle">Antialiasing</h3>
<p>We experimented with antialiasing by taking more renders (higher NxN), and found that this improved final image quality significantly.</p>
<div align="center">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/33.png" align="middle" width="360px"/>
      </td>
      <td>
        <img src="images/55.png" align="middle" width="360px"/>
     </td>
      <td>
        <img src="images/99.png" align="middle" width="360px"/>
     </td>
    </tr>
  </table>
</div>

<h3 align="middle">Problems Encountered</h3>
<ul>
  <li>Initially, we were taking one render and cropping it to generate subpixel images, and were frustrated that none of our composed images were focusing at the right locations. We realized that, after taking another look at the lecture slides, the subpixel images are taken at slightly different camera angles, so we would have to render an image for each camera angle, which made a huge difference compared to cropping (the composed image would actually focus)!</li>
  <li>Image processing was too slow at first - but after switching to NumPy, we saw a significant speed improvement.</li>
  <li>The Hive machines would occassionally crash, so Denis had to monitor it at a higher frequency (once every 2 hours)</li>
</ul>

<h3 align="middle">Lessons Learned</h3>
<ul>
  <li>For any project involving image manipulation, leave a large portion of time for waiting on renders/image processing, as this consumed most of our time working on this project.</li>
  <li>Always implement optimizations if you have time! NumPy and other libraries can help!</li>
</ul>

<h2 align="middle">Results</h2>

<h2 align="middle">References</h2>
<p>Professor Ren Ng for his slides on light field cameras: <a href="https://cs184.eecs.berkeley.edu/lecture/lfcamera-1">1</a> <a href="https://cs184.eecs.berkeley.edu/lecture/lfcamera-2">2</a> <a href="https://cs184.eecs.berkeley.edu/lecture/lfcamera-3">3</a></p>

<h2 align="middle">Contributions</h2>
<h3 align="left">Keming Kao</h3>
<ul>
  <li>Added support for shifting camera position via command line arguments</li>
  <li>Printed out camera and shift parameters for Denis to render on hive machines</li>
</ul>
<h3 align="left">Denis Li</h3>
<ul>
  <li>Wrote initial image manipulation functions in Python</li>
  <li>Set up all renders on hive machines and monitored them / kept them running</li>
</ul>
<h3 align="left">David Zhang</h3>
<ul>
  <li>Optimized image manipulation code using NumPy</li>
  <li>Made GUI and sliders to demonstrate final results</li>
</ul>

<a href="https://docs.google.com/presentation/d/1cFkqdM90xR6Ynfzc4gZclph1cVCHKON2bfyBnAvOjHI/edit?usp=sharing">Link to slides</a>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/-XcOih2xROA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
