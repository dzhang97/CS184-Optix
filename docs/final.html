<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
</style>
<title>CS 184 Optix</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">
</head>


<body>

<h1 align="middle">CS 184: Computer Graphics and Imaging, Spring 2018</h1>
<h1 align="middle">Final Report Webpage</h1>
<h2 align="middle">Keming Kao, Denis Li, David Zhang</h2>

<h2 align="middle"></h2>

<h2 align="middle">Nvidia OptiX as a Tool to Accelerate Path Tracing</h2>

<h2 align="middle">Light Field Cameras</h2>
<p>As a group, we agreed to change our project completely from our original plan (Nvidia OptiX to optimize path tracing), as we did not realize that using the Nvidia OptiX API would require having a Nvidia GPU in our system. One group member did have a GPU, but building the pathtracer project on Windows proved to be a whole project on its own, so we decided to switch to the light field camera project that built off the existing pathtracer code. Our objective was to emulate the multilens array shown in lecture to simulate a plenoptic camera. To do so, we had to make the assumption that pinhole images looked like subpixel images, and leveraged command line arguments for easy camera shifting. Using our grid of images, we could recreate images using Python and the shift and add algorithm presented in class. Playing around with this allowed for refocusing at different depths. As a final touch, we implemented antialiasing by taking a larger grid of subpixel images for the final, summed image.</p>

<h2 align="middle">Technical Approach</h2>

<h3 align="middle">Varying Camera Position</h3>
<p>To achieve multiple shots in a localized area through our pinhole camera model, we needed an efficient way to render multiple images on multiple machines simultaneously, so one of the first goals we accomplished was enabling command line arguments for changing the camera's position and its target location (where it was pointing at). Through some simple code digging we created simple calls from main to application to camera that set the spherical coordinates of the camera and target location, as well as supporting printing out these coordinates in the edit mode of the renderer. We also printed out the X and Y normals to the camera direction from the c2w member of camera to be able to add shifts to our camera position while being on the same XY plane.</p>

<h2 align="middle">Microlenses and Sub-Aperture Images</h2>
<p>With the ability to set up renders in a small, localized area in the input image, we could now simulate a microlens by taking individual shots in a NxN grid with set offsets. The offsets were generated by shifting by some set amount in the X normal and Y normal directions.</p>
<div align="center">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/sal.png" align="middle" width="360px"/>
          <figcaption align="middle">This would be a render taken slightly to the left</figcaption>
      </td>
      <td>
        <img src="images/sar.png" align="middle" width="360px"/>
          <figcaption align="middle">This would be a render taken slightly to the right</figcaption>
     </td>
    </tr>
  </table>
</div>
<p><i>[Image courtesy of Light Field Cameras II Lecture]</i></p>


<h3 align="middle">Focusing and Depth of Field</h3>
<p>We found that with just a 3x3 example, we were able to accomplish the "shift and add" algorithm explained in lecture to focus near or far in the scene. To get a more precise and adjustable focus, we leveraged bilinear interpolation between the shifts to achieve non-integer shift results. In general, shifting inwards gave a closer focus and shifting outwards gave a further focus. We experimented with different small offsets to test their effects on simulated aperture size and depth of field. Judging by the blurs in the sum of our NxN renders, we found that smaller shifts in the images generated a small aperture and large depth of field, while larger shifts generated a large aperture and small depth of field. Examples of our current work are linked below.</p>

<h3 align="middle">Antialiasing</h3>
<p>We experimented with antialiasing by taking more renders (higher NxN), and found that this improved final image quality significantly.</p>
<div align="center">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/33.png" align="middle" width="360px"/>
      </td>
      <td>
        <img src="images/55.png" align="middle" width="360px"/>
     </td>
      <td>
        <img src="images/99.png" align="middle" width="360px"/>
     </td>
    </tr>
  </table>
</div>



<a href="https://docs.google.com/presentation/d/1cFkqdM90xR6Ynfzc4gZclph1cVCHKON2bfyBnAvOjHI/edit?usp=sharing">Link to slides</a>
<iframe width="560" height="315" src="https://www.youtube.com/embed/Mh_OAZ4roFg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
