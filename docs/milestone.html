<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
</style>
<title>CS 184 Optix</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">
</head>


<body>

<h1 align="middle">CS 184: Computer Graphics and Imaging, Spring 2018</h1>
<h1 align="middle">Final Project Milestone</h1>
<h2 align="middle">Keming Kao, Denis Li, David Zhang</h2>

<h2 align="middle"></h2>

<h2 align="middle">Nvidia OptiX as a Tool to Accelerate Path Tracing</h2>
<p>As a group, we agreed to change our project completely, as we did not realize that using the Nvidia Optix API would require having a Nvidia GPU in our system. One group member did have a GPU, but building the pathtracer project on Windows proved to be a whole project on its own, so we decided to switch to the light field camera project that extends pathtracer, which supports refocusing after rendering.</p>

<div align="center">
  <img src="images/gpumeme.jpg" align="middle" width="240px"/>
</div>

<h2 align="middle">Light Field Cameras</h2>
<p></p>

<h2 align="middle">Varying Camera Position</h2>
<p>To achieve multiple shots in a localized area through our pinhole camera model, we needed an efficient way to render multiple images on multiple machines simultaneously, so one of the first goals we accomplished was enabling command line arguments for changing the camera's position and its target location (where it was pointing at). Through some simple code digging we created simple calls from main to application to camera that set the spherical coordinates of the camera and target location, as well as supporting printing out these coordinates in the edit mode of the renderer. We also printed out the X and Y normals to the camera direction from the c2w member of camera to be able to add shifts to our camera position while being on the same XY plane.</p>

<h2 align="middle">Microlenses and Sub-Aperture Images</h2>
<p>With the ability to set up renders in a small, localized area in the input image, we could now simulate a microlens by taking individual shots in a NxN grid with set offsets. The offsets were generated by shifting by some set amount in the X normal and Y normal directions.</p>
<div align="center">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/sal.png" align="middle" width="360px"/>
          <figcaption align="middle">This would be a render taken slightly to the left</figcaption>
      </td>
      <td>
        <img src="images/sar.png" align="middle" width="360px"/>
          <figcaption align="middle">This would be a render taken slightly to the right</figcaption>
     </td>
    </tr>
  </table>
</div>
<p><i>[Image courtesy of Light Field Cameras II Lecture]</i></p>

<h2 align="middle">Focusing and Depth of Field</h2>
<p>We found that with just a 3x3 example, we were able to accomplish the "shift and add" algorithm explained in lecture to focus near or far in the scene. To get a more precise and adjustable focus, we leveraged bilinear interpolation between the shifts to achieve non-integer shift results. In general, shifting inwards gave a closer focus and shifting outwards gave a further focus. We experimented with different small offsets to test their effects on simulated aperture size and depth of field. Judging by the blurs in the sum of our NxN renders, we found that smaller shifts in the images generated a small aperture and large depth of field, while larger shifts generated a large aperture and small depth of field. Examples of our current work are linked below.</p>

<h2 align="middle">Antialiasing</h2>
<p>We experimented with antialiasing by taking more renders (higher NxN), but are still working on changing depth of field for varying NxN cases for fair comparison.</p>

<h2 align="middle">Progress</h2>
<p>Overall, we are satisfied with our progress considering we had to change our project idea completely, and will be completed after tuning antialiasing.</p>

<a href="https://docs.google.com/presentation/d/1cFkqdM90xR6Ynfzc4gZclph1cVCHKON2bfyBnAvOjHI/edit?usp=sharing">Link to slides</a>
<iframe width="560" height="315" src="https://www.youtube.com/embed/Mh_OAZ4roFg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
